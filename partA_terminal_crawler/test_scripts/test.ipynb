{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox Area for testing code snip-its\n",
    "DOCS\n",
    "\n",
    "https://praw.readthedocs.io/en/stable/getting_started/quick_start.html#submission-iteration  \n",
    "\n",
    "https://praw.readthedocs.io/en/stable/tutorials/comments.html  \n",
    "\n",
    "https://stackoverflow.com/questions/4041238/why-use-def-main  \n",
    "\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768  \n",
    "\n",
    "https://stackoverflow.com/questions/30734682/extracting-url-and-anchor-text-from-markdown-using-python  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find attributes in praw object !\n",
    "\n",
    "https://praw.readthedocs.io/en/stable/getting_started/quick_start.html#determine-available-attributes-of-an-object  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# To create a read-only Reddit instance, you need three pieces of information:\n",
    "# Client ID\n",
    "# Client secret\n",
    "# User agent\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"3IS_PPpIX3IkVIh-1f8cHQ\",\n",
    "client_secret=\"AvAKHMywUgLyGlVSVD0bQMRpZbhb1w\",\n",
    "user_agent =\"crawler/scrapper\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabing Specfic Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\":\"[D] Google \\\"We Have No Moat, And Neither Does OpenAI\\\": Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI\",\n",
      "        \"score\":661,\n",
      "        \"id\":\"137rxgw\",\n",
      "        \"url\":\"https:\\/\\/www.semianalysis.com\\/p\\/google-we-have-no-moat-and-neither\",\n",
      "        \"num_comments\":113,\n",
      "        \"body\":\"\",\n",
      "        \"created\":1683216810.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[R] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\",\n",
      "        \"score\":27,\n",
      "        \"id\":\"1381gd3\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1381gd3\\/r_distilling_stepbystep_outperforming_larger\\/\",\n",
      "        \"num_comments\":8,\n",
      "        \"body\":\"paper:  [\\\\[2305.02301\\\\] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arxiv.org)](https:\\/\\/arxiv.org\\/abs\\/2305.02301) \\n\\nAbstract: \\n\\n> Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled\\/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80% of available data on a benchmark task.\",\n",
      "        \"created\":1683237630.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[R] Fully Autonomous Programming with Large Language Models\",\n",
      "        \"score\":25,\n",
      "        \"id\":\"137odqz\",\n",
      "        \"url\":\"https:\\/\\/arxiv.org\\/abs\\/2304.10423\",\n",
      "        \"num_comments\":7,\n",
      "        \"body\":\"\",\n",
      "        \"created\":1683212038.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[Discussion]: Mark Zuckerberg on Meta's Strategy on Open Source and AI during the earnings call\",\n",
      "        \"score\":388,\n",
      "        \"id\":\"1373nhq\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1373nhq\\/discussion_mark_zuckerberg_on_metas_strategy_on\\/\",\n",
      "        \"num_comments\":79,\n",
      "        \"body\":\"During  the recent earnings call, Mark Zuckerberg answered a question from Eric  Sheridan of Goldman Sachs on Meta's AI strategy, opportunities to  integrate into products, and why they open source models and how it  would benefit their business.\\n\\nI found the reasoning to be very sound and promising for the OSS and AI community.\\n\\nThe  biggest risk from AI, in my opinion, is not the doomsday scenarios that  intuitively come to mind but rather that the most powerful AI systems  will only be accessible to the most powerful and resourceful  corporations.\\n\\nQuote copied from Ben Thompson's write up on Meta's earning in his [Stratechery blog post](https:\\/\\/stratechery.com\\/2023\\/facebook-earnings-generative-ai-and-messaging-monetization-open-source-and-ai\\/) which goes beyond AI. *It's behind a paywall but I highly recommend it personally.*\\n\\nSome noteworthy quotes that signal the thought process at Meta FAIR and more broadly\\n\\n* We\\u2019re just playing a different game on the infrastructure  than companies like Google or Microsoft or Amazon\\n* We would aspire to and hope to make even more open than that. So, we\\u2019ll need to figure out a way to do that.\\n* ...lead us to do more work in terms of open sourcing, some of the lower level models and tools\\n* Open sourcing low level tools make the way we run all this infrastructure more efficient over time.\\n* On  PyTorch: It\\u2019s generally been very valuable for us to provide that  because now  all of the best developers across the industry are using  tools that  we\\u2019re also using internally.\\n* I would expect us to be pushing and helping  to build out an open ecosystem.\\n\\nFor  all the negative that comes out of the popular discourse on Meta, I  think their work to open source key tech tools over the last 10 years  has been exceptional, here's hoping it continues into this decade of AI  and pushes other tech giants to also realize the benefits of Open  Source.\\n\\nFull Transcript:\\n\\n>Right  now most of the companies that are training large language  models have  business models that lead them to a closed approach to development. I  think **there\\u2019s an** **important opportunity to help create an  open ecosystem.**  If we can help be a part of this, then much of the  industry will  standardize on using these open tools and help improve  them further. So  this will make it easier for other companies to  integrate with our  products and platforms as we enable more  integrations, and that will  help our products stay at the leading edge  as well.  \\nOur  approach to AI and our infrastructure has always been fairly  open. We  open source many of our state of the art models so people can   experiment and build with them. This quarter we released our LLaMa LLM   to researchers. It has 65 billion parameters but outperforms larger   models and has proven quite popular. We\\u2019ve also open-sourced three other   groundbreaking visual models along with their training data and model   weights \\u2014 Segment Anything, DinoV2, and our Animated Drawings tool \\u2014  and  we\\u2019ve gotten positive feedback on all of those as well.  \\nI  think that there\\u2019s an important distinction between the products we  offer and a lot of the technical infrastructure, especially the software  that we write to support that. And historically, whether it\\u2019s the Open  Compute project that we\\u2019ve done or just open sourcing a lot of the   infrastructure that we\\u2019ve built, we\\u2019ve historically open sourced a lot   of that infrastructure, even though we haven\\u2019t open sourced the code for   our core products or anything like that.  \\nAnd the reason why I think why we do this is that unlike some of  the other companies in the space, **we\\u2019re not selling a cloud computing service** **where we try to keep the different software infrastructure that we\\u2019re building proprietary.** For us, **it\\u2019s way better if the industry  standardizes on the basic tools that we\\u2019re using**  and therefore we can benefit from the improvements that others make and  others\\u2019 use of those tools can, in some cases like Open Compute, **drive down the costs** of  those things which make our business more efficient too. So I think to  some degree **we\\u2019re just playing a different game** on the infrastructure  than companies like Google or Microsoft or Amazon, and that creates different incentives for us.  \\nSo overall, I think **that that\\u2019s going to lead us to do more work in terms of open sourcing, some of the lower level models and tools**.  But of  course, a lot of the product work itself is going to be  specific and  integrated with the things that we do. So it\\u2019s not that  everything we do is going to be open. Obviously, a bunch of this needs  to be developed in a way that creates unique value for our products, but  I think in  terms of the basic models, **I would expect us to be pushing and helping  to build out an open ecosystem** here, which I think is something that\\u2019s  going to be important.  \\nOn the AI tools, and we have a bunch of history here, right? So if you  if you look at what we\\u2019ve done with **PyTorch**,  for example, which has  generally become the standard in the industry  as a tool that a lot of  folks who are building AI models and different  things in that space use,  **it\\u2019s generally been very valuable** for us to provide that because now  all of the **best developers across the industry are using tools that  we\\u2019re also using internally**.  So the tool chain is the same. So when they create some innovation, we  can easily integrate it into the things that we\\u2019re doing. When we  improve something, it improves other products too. Because it\\u2019s  integrated with our technology stack, when there are opportunities to  make integrations with products, it\\u2019s much easier to  make sure that  developers and other folks are compatible with the things  that we need  in the way that our systems work.  \\nSo there are a lot of advantages, but **I view this more as a kind of back end infrastructure advantage with potential integrations on the  product side**,  but one that should hopefully enable us to stay at the  leading edge  and integrate more broadly with the community and also make  the way we  run all this infrastructure more efficient over time. There  are a  number of models. I just gave PyTorch as an example. Open Compute  is  another model that has worked really well for us in this way, both to   incorporate both innovation and scale efficiency into our own   infrastructure.  \\nSo I think that  there\\u2019s, our incentives I think are basically  aligned towards moving in  this direction. Now that said, there\\u2019s a lot  to figure out, right? So  when you asked if there are going to be other opportunities, I hope so. I  can\\u2019t speak to what all those things might  be now. This is all quite  early in getting developed. **The better we do at the foundational work, the more opportunities** I think that will come and present themselves. So I think that that\\u2019s all stuff that we need to  figure out. But at least **at the base level, I think we\\u2019re generally incentivized to move in this direction**. And we also need to figure out  how to go in that direction over time.  \\nI  mean, I mentioned LLaMA before and I also want to be clear that  while  I\\u2019m talking about helping contribute to an open ecosystem, LLaMA  is a  model that we only really made available to researchers and there\\u2019s  a  lot of really good stuff that\\u2019s happening there. But a lot of the  work  that we\\u2019re doing, I think, **we would aspire to and hope to make even more open than that. So, we\\u2019ll need to figure out a way to do that.**\",\n",
      "        \"created\":1683157697.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"Prediction and Entropy of Printed English. Shannon 1951\",\n",
      "        \"score\":7,\n",
      "        \"id\":\"137zz3j\",\n",
      "        \"url\":\"https:\\/\\/archive.org\\/details\\/bstj30-1-50\",\n",
      "        \"num_comments\":2,\n",
      "        \"body\":\"This is a great and easily read paper. LLMs do the task described here really well. And I didn't realize how useful that could be.\",\n",
      "        \"created\":1683234359.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] What tech stacks do you use when creating an LLM based app?\",\n",
      "        \"score\":2,\n",
      "        \"id\":\"1386pjt\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1386pjt\\/d_what_tech_stacks_do_you_use_when_creating_an\\/\",\n",
      "        \"num_comments\":0,\n",
      "        \"body\":\"Making apps based on foundational LLMs feels like it should have a tech stack \\\"pattern\\\" - a commonly used set of tools that most of the apps use unless there's a unique reason to deviate. \\n\\nWhat tech stacks do people here use?\\n\\nThe link below has some suggestions, but it would be great to know what people use in practice. Are these ones good?\\n\\n[https:\\/\\/gradientflow.com\\/building-llm-powered-apps-what-you-need-to-know\\/](https:\\/\\/gradientflow.com\\/building-llm-powered-apps-what-you-need-to-know\\/)\",\n",
      "        \"created\":1683250596.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] Can biological neurons be properly emulated with current microcomputer hardware?\",\n",
      "        \"score\":1,\n",
      "        \"id\":\"13890af\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/13890af\\/d_can_biological_neurons_be_properly_emulated\\/\",\n",
      "        \"num_comments\":0,\n",
      "        \"body\":\"I've been doing some browsing on how neurons work and what follows is the conclusion I've come to.\\n\\nThe functionality of biological neurons is impossible to emulate with current microcomputing technology. This is because biological neurons have 2 important features that are expensive to imitate:\\n\\n1.\\tIt is possible for any two biological neurons to connect. Since their cell body, along with their axons and dendrites, is able to move freely, two correlated neurons will eventually find and connect to each other if given enough time. The only way to mimic this behavior in a single-processor computer without sacrificing time is by making a fully connected graph of the neurons, which is awful because it requires n\\\\^2 space.\\n\\n2.\\tEach neuron operates in parallel. This means increases in number of neurons only require more mass, which is much more freely available than the extra time that a single-processor computer would need to add the same number of neurons. For instance, the human brain has \\\\~86 billion neurons. Assuming a 1 GHz oscillator, and that each neuron only requires 1 cycle to calculate its value, a single-processor computer would still take a whole 8.6 seconds to calculate the state of the brain after 1 time step. The human brain runs the same time step in, well, much less time than that.\\n\\nSo basically, in order to emulate a brain, a single-processor computer would have to make some tradeoff between n\\\\^2 space and n time, neither of which can be afforded.\\n\\nThoughts?\",\n",
      "        \"created\":1683256415.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[Research] Towards Accurate, Credible and Traceable Large Language Models\\uff01\\uff01\\uff01\",\n",
      "        \"score\":11,\n",
      "        \"id\":\"137iyxk\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/137iyxk\\/research_towards_accurate_credible_and_traceable\\/\",\n",
      "        \"num_comments\":4,\n",
      "        \"body\":\"Hello everyone, in this paper, we propose a novel method to combine Large Language Models with Information Retrieval  to improve the accuracy, credibility and traceability of LLM-generated content!\\n\\nPaper: [https:\\/\\/arxiv.org\\/abs\\/2304.14732](https:\\/\\/arxiv.org\\/abs\\/2304.14732)\\n\\n&#x200B;\\n\\nhttps:\\/\\/preview.redd.it\\/t5kdmrna3txa1.png?width=1431&format=png&auto=webp&v=enabled&s=fa52e9bd9f9d5ae892509f551f1ef63234bb77ff\",\n",
      "        \"created\":1683202882.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[Research] [Project] Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model\",\n",
      "        \"score\":4,\n",
      "        \"id\":\"137ssn6\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/137ssn6\\/research_project_texttoaudio_generation_using\\/\",\n",
      "        \"num_comments\":2,\n",
      "        \"body\":\"Paper: [https:\\/\\/arxiv.org\\/abs\\/2304.13731](https:\\/\\/arxiv.org\\/abs\\/2304.13731)\\n\\nCode: [https:\\/\\/github.com\\/declare-lab\\/tango](https:\\/\\/github.com\\/declare-lab\\/tango)\\n\\nDemo: [https:\\/\\/huggingface.co\\/spaces\\/declare-lab\\/tango](https:\\/\\/huggingface.co\\/spaces\\/declare-lab\\/tango)\\n\\nProject: [https:\\/\\/tango-web.github.io\\/](https:\\/\\/tango-web.github.io\\/)\\n\\nAbstract: The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM FLAN-T5 as the text encoder for text-to audio (TTA) generation\\u2014a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach (TANGO) outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level based sound mixing for training set augmentation, whereas the prior methods take a random mix.  \\n\\n\\nhttps:\\/\\/preview.redd.it\\/uzioyoqpfuxa1.png?width=7784&format=png&auto=webp&v=enabled&s=e667cfa557f77552c6657a799edb651ee7febf6c\",\n",
      "        \"created\":1683218687.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] Oblivus Cloud | Scalable GPU servers from $0.29\\/hr\",\n",
      "        \"score\":119,\n",
      "        \"id\":\"1370xg9\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1370xg9\\/d_oblivus_cloud_scalable_gpu_servers_from_029hr\\/\",\n",
      "        \"num_comments\":17,\n",
      "        \"body\":\"Greetings\\u00a0r\\/MachineLearning!\\n\\nThis is Doruk from Oblivus, and I'm excited to announce the launch of our platform, Oblivus Cloud. After more than a year of beta testing, we're excited to offer you a platform where you can deploy affordable and scalable GPU virtual machines in as little as 30 seconds! We believe that Oblivus Cloud is the perfect alternative to other cloud service providers when it comes to training your ML models.\\n\\n[https:\\/\\/oblivus.com\\/cloud](https:\\/\\/oblivus.com\\/cloud)\\n\\n\\ud83e\\udd14\\u00a0**What sets Oblivus Cloud apart?**\\n\\nAt the start of our journey, we had two primary goals in mind: to democratize High-Performance Computing and make it as straightforward as possible. We understand that maintaining GPU servers through major cloud service providers can be expensive, with hidden fees adding to the burden of running and maintaining servers.\\n\\nAdditionally, the cloud can sometimes be overly complex for individuals who don't have much knowledge but still require powerful computing resources.\\n\\nThat's why we decided to create a platform that offers affordable pricing, easy usability, and high-quality performance. Oblivus Cloud provides just that - a simple, affordable, and high-quality alternative for anyone in need of powerful computing resources.\\n\\n\\u26aa\\u00a0**Features**\\n\\nOblivus Cloud comes packed with a wide range of features to make your experience smooth, seamless, and fully customizable. Here are some of the key features you can expect:\\n\\n1. Fully customizable infrastructure that lets you switch between CPU and GPU configurations to suit your needs. You can easily modify server components and scale your virtual machine up and down in seconds.\\n2. No quotas or complex verification processes. Whether you represent a company, an institution, or you're a researcher, you have full access to our infrastructure without any limitations.\\n3. Each virtual machine comes with 10Gbps to 40Gbps public network connectivity.\\n4. Transparent and affordable per-minute-based Pay-As-You-Go pricing with no hidden fees. Plus, free data ingress and egress. (Pricing:\\u00a0[https:\\/\\/oblivus.com\\/pricing\\/](https:\\/\\/oblivus.com\\/pricing\\/))\\n5. Optimized cost with storage and IP address-only billing when the virtual machine is shut down.\\n6. NVMe ($0.00011\\/GB\\/hr) and HDD ($0.00006\\/GB\\/hr) local and network storage that is 3x replicated to fulfill your storage needs.\\n7. Choose from a variety of cutting-edge CPUs and 10 state-of-the-art GPU SKUs. (Availability:\\u00a0[https:\\/\\/oblivus.com\\/availability\\/](https:\\/\\/oblivus.com\\/availability\\/))\\n8. Access our infrastructure from three data center locations in Chicago, New York City, and Las Vegas. (Data Centers:\\u00a0[https:\\/\\/oblivus.com\\/datacenters\\/](https:\\/\\/oblivus.com\\/datacenters\\/))\\n9. OblivusAI OS images come with pre-installed ML libraries, so you can start training your models right away without the hassle of installing and configuring the necessary libraries.\\n10. If you're working with a team, utilize our organization feature to simplify the billing process. Everyone in your organization uses the same billing profile, so you don't need to keep track of multiple accounts.\\n11. Easy-to-use API with detailed documentation so that you can integrate your code with ours.\\n12. In addition to on-demand servers, we also offer Reserved Instances if your computing needs don't change often, giving you access to more discounts.\\n\\n\\ud83d\\udcb2\\u00a0**Pricing**\\n\\nAt Oblivus Cloud, we provide pricing that is affordable, transparent, and up to 80% cheaper than major cloud service providers, while still offering the computing power you need for your machine learning models. Here is a breakdown of our pricing:\\n\\n1. CPU-based virtual machines starting from just $0.019\\/hour.\\n2. NVIDIA Quadro RTX 4000s starting from $0.27\\/hour.\\n3. Tesla V100s starting from $0.51\\/hour.\\n4. NVIDIA A40s and RTX A6000s starting from $1.41\\/hour.\\n5. NVIDIA A100s starting from $2.25\\/hour.\\n\\nWe also offer 5 other GPU SKUs to help you accurately size your workloads and only pay for what you need. Say goodbye to hidden fees and unpredictable costs.\\n\\nIf you represent a company, be sure to register for a business account to access even better pricing rates. ([https:\\/\\/console.oblivus.com\\/business\\/](https:\\/\\/console.oblivus.com\\/business\\/))\\n\\n\\ud83c\\udf8a\\u00a0**Promo Code**\\n\\nJoin us in celebrating the launch of Oblivus Cloud by claiming your $1 free credit! This may sound small, but it's enough to get started with us and experience the power of our platform. With $1, you can get over 3 hours of computing on our most affordable GPU-based configuration, or over 50 hours of computing on our cheapest CPU-based configuration.\\n\\nTo redeem this free credit, simply use the code REDDIT\\\\_1 on the 'Add Balance' page after registration.\\n\\nRegister now at\\u00a0[https:\\/\\/console.oblivus.com\\/register](https:\\/\\/console.oblivus.com\\/register)\\n\\n\\ud83d\\udd17\\u00a0**Quick Links**\\n\\nWebsite:\\u00a0[https:\\/\\/oblivus.com\\/](https:\\/\\/oblivus.com\\/)\\n\\nConsole:\\u00a0[https:\\/\\/console.oblivus.com\\/](https:\\/\\/console.oblivus.com\\/)\\n\\nCompany Documentation:\\u00a0[https:\\/\\/docs.oblivus.com\\/](https:\\/\\/docs.oblivus.com\\/)\\n\\nAPI Documentation:\\u00a0[https:\\/\\/documenter.getpostman.com\\/view\\/21699896\\/UzBtoQ3e](https:\\/\\/documenter.getpostman.com\\/view\\/21699896\\/UzBtoQ3e)\\n\\nIf you have any questions, feel free to post them below and I'll be happy to assist you.\",\n",
      "        \"created\":1683151148.0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://www.reddit.com/r/funny/comments/3g1jfi/buttons/\"\n",
    "submission = reddit.submission(url=url)\n",
    "\n",
    "posts = []\n",
    "\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "# for post in ml_subreddit.hot(limit=10):\n",
    "#     posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "# df = pd.DataFrame(posts, columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "# json = df.to_json(orient='records', indent=4)\n",
    "# print(json)\n",
    "\n",
    "# for post in ml_subreddit.new(limit=10):\n",
    "#     posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "# df = pd.DataFrame(posts, columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "# json = df.to_json(orient='records', indent=4)\n",
    "# print(json)\n",
    "\n",
    "for post in ml_subreddit.rising(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "df = pd.DataFrame(posts, columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "json = df.to_json(orient='records', indent=4)\n",
    "print(json)\n",
    "\n",
    "# import json\n",
    "# js = json.dumps(posts)\n",
    "# print(js)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabin Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit will soon only be available over HTTPS\n",
      "{'_comments': <praw.models.comment_forest.CommentForest object at 0x000002A843E65C90>,\n",
      " '_comments_by_id': {'t1_cs7vwlm': Comment(id='cs7vwlm'),\n",
      "                     't1_cs7xcx2': Comment(id='cs7xcx2'),\n",
      "                     't1_cs7ykx6': Comment(id='cs7ykx6'),\n",
      "                     't1_cs81mem': Comment(id='cs81mem'),\n",
      "                     't1_cs81xp8': Comment(id='cs81xp8'),\n",
      "                     't1_cs82epc': Comment(id='cs82epc'),\n",
      "                     't1_cs82kes': Comment(id='cs82kes'),\n",
      "                     't1_cs83dd8': Comment(id='cs83dd8'),\n",
      "                     't1_cs83ua8': Comment(id='cs83ua8'),\n",
      "                     't1_cs83xhc': Comment(id='cs83xhc'),\n",
      "                     't1_cs846jk': Comment(id='cs846jk'),\n",
      "                     't1_cs847yp': Comment(id='cs847yp'),\n",
      "                     't1_cs848n2': Comment(id='cs848n2'),\n",
      "                     't1_cs84apf': Comment(id='cs84apf'),\n",
      "                     't1_cs84kz5': Comment(id='cs84kz5'),\n",
      "                     't1_cs84n50': Comment(id='cs84n50'),\n",
      "                     't1_cs84poe': Comment(id='cs84poe'),\n",
      "                     't1_cs84rjx': Comment(id='cs84rjx'),\n",
      "                     't1_cs85lql': Comment(id='cs85lql'),\n",
      "                     't1_cs87jic': Comment(id='cs87jic'),\n",
      "                     't1_cs898tl': Comment(id='cs898tl'),\n",
      "                     't1_cs89jvc': Comment(id='cs89jvc'),\n",
      "                     't1_cs8a7pr': Comment(id='cs8a7pr'),\n",
      "                     't1_cs8blse': Comment(id='cs8blse'),\n",
      "                     't1_cs8feqw': Comment(id='cs8feqw'),\n",
      "                     't1_cs8hwfi': Comment(id='cs8hwfi'),\n",
      "                     't1_cs8i7od': Comment(id='cs8i7od'),\n",
      "                     't1_cs8iofx': Comment(id='cs8iofx'),\n",
      "                     't1_cs8j88h': Comment(id='cs8j88h'),\n",
      "                     't1_cs8jh7g': Comment(id='cs8jh7g'),\n",
      "                     't1_cs8jkru': Comment(id='cs8jkru'),\n",
      "                     't1_cs8jns1': Comment(id='cs8jns1'),\n",
      "                     't1_cs8jq4l': Comment(id='cs8jq4l'),\n",
      "                     't1_cs8kbbq': Comment(id='cs8kbbq'),\n",
      "                     't1_cs8kc4z': Comment(id='cs8kc4z'),\n",
      "                     't1_cs8kf07': Comment(id='cs8kf07'),\n",
      "                     't1_cs8km94': Comment(id='cs8km94'),\n",
      "                     't1_cs8knnt': Comment(id='cs8knnt'),\n",
      "                     't1_cs8koyi': Comment(id='cs8koyi'),\n",
      "                     't1_cs8kwd0': Comment(id='cs8kwd0'),\n",
      "                     't1_cs8kxvj': Comment(id='cs8kxvj'),\n",
      "                     't1_cs8l1wk': Comment(id='cs8l1wk'),\n",
      "                     't1_cs8lkbl': Comment(id='cs8lkbl'),\n",
      "                     't1_cs8llxd': Comment(id='cs8llxd'),\n",
      "                     't1_cs8lso1': Comment(id='cs8lso1'),\n",
      "                     't1_cs8mez7': Comment(id='cs8mez7'),\n",
      "                     't1_cs8mhlc': Comment(id='cs8mhlc'),\n",
      "                     't1_cs8p6nn': Comment(id='cs8p6nn'),\n",
      "                     't1_cs8t8u9': Comment(id='cs8t8u9'),\n",
      "                     't1_cs8ts1k': Comment(id='cs8ts1k'),\n",
      "                     't1_cs8x1s6': Comment(id='cs8x1s6'),\n",
      "                     't1_cs905rb': Comment(id='cs905rb'),\n",
      "                     't1_cs974f0': Comment(id='cs974f0'),\n",
      "                     't1_cs9daoa': Comment(id='cs9daoa'),\n",
      "                     't1_cs9dn0k': Comment(id='cs9dn0k'),\n",
      "                     't1_cs9dr4n': Comment(id='cs9dr4n'),\n",
      "                     't1_cs9e0cr': Comment(id='cs9e0cr'),\n",
      "                     't1_cs9ecqc': Comment(id='cs9ecqc'),\n",
      "                     't1_cs9emjk': Comment(id='cs9emjk'),\n",
      "                     't1_cs9gpm0': Comment(id='cs9gpm0'),\n",
      "                     't1_cs9i9q3': Comment(id='cs9i9q3'),\n",
      "                     't1_cs9l636': Comment(id='cs9l636'),\n",
      "                     't1_cs9nc77': Comment(id='cs9nc77'),\n",
      "                     't1_cs9ndy5': Comment(id='cs9ndy5'),\n",
      "                     't1_cs9no26': Comment(id='cs9no26'),\n",
      "                     't1_cs9nweg': Comment(id='cs9nweg'),\n",
      "                     't1_cs9rfqm': Comment(id='cs9rfqm'),\n",
      "                     't1_cs9tp7a': Comment(id='cs9tp7a'),\n",
      "                     't1_cs9tso1': Comment(id='cs9tso1'),\n",
      "                     't1_cs9z8jp': Comment(id='cs9z8jp'),\n",
      "                     't1_csa3tdo': Comment(id='csa3tdo'),\n",
      "                     't1_csa5ckt': Comment(id='csa5ckt'),\n",
      "                     't1_csa7guj': Comment(id='csa7guj'),\n",
      "                     't1_csaf2xj': Comment(id='csaf2xj'),\n",
      "                     't1_csaifdc': Comment(id='csaifdc'),\n",
      "                     't1_csalcjw': Comment(id='csalcjw'),\n",
      "                     't1_csapdsx': Comment(id='csapdsx'),\n",
      "                     't1_csaqqg3': Comment(id='csaqqg3'),\n",
      "                     't1_csav2lc': Comment(id='csav2lc'),\n",
      "                     't1_csay9va': Comment(id='csay9va'),\n",
      "                     't1_csaym10': Comment(id='csaym10'),\n",
      "                     't1_csaz9yt': Comment(id='csaz9yt'),\n",
      "                     't1_csazbjn': Comment(id='csazbjn'),\n",
      "                     't1_csb01td': Comment(id='csb01td'),\n",
      "                     't1_csb0xnn': Comment(id='csb0xnn'),\n",
      "                     't1_csb2iss': Comment(id='csb2iss'),\n",
      "                     't1_csbxpsm': Comment(id='csbxpsm'),\n",
      "                     't1_csc3ini': Comment(id='csc3ini'),\n",
      "                     't1_csghdd8': Comment(id='csghdd8'),\n",
      "                     't1_csh2ggk': Comment(id='csh2ggk'),\n",
      "                     't1_cshasj0': Comment(id='cshasj0'),\n",
      "                     't1_csjbsq9': Comment(id='csjbsq9'),\n",
      "                     't1_cso07fm': Comment(id='cso07fm'),\n",
      "                     't1_csocuyr': Comment(id='csocuyr'),\n",
      "                     't1_csomokx': Comment(id='csomokx'),\n",
      "                     't1_csoncci': Comment(id='csoncci'),\n",
      "                     't1_csoqfop': Comment(id='csoqfop'),\n",
      "                     't1_csoqwaj': Comment(id='csoqwaj'),\n",
      "                     't1_csotkw8': Comment(id='csotkw8'),\n",
      "                     't1_csovx2n': Comment(id='csovx2n'),\n",
      "                     't1_csow6zt': Comment(id='csow6zt'),\n",
      "                     't1_csoycql': Comment(id='csoycql'),\n",
      "                     't1_csp2uvk': Comment(id='csp2uvk'),\n",
      "                     't1_csrxl8t': Comment(id='csrxl8t'),\n",
      "                     't1_csrxya2': Comment(id='csrxya2'),\n",
      "                     't1_cswg4ku': Comment(id='cswg4ku'),\n",
      "                     't1_cswg9cd': Comment(id='cswg9cd'),\n",
      "                     't1_cswk16a': Comment(id='cswk16a'),\n",
      "                     't1_ct3lvx5': Comment(id='ct3lvx5'),\n",
      "                     't1_ct4hrif': Comment(id='ct4hrif'),\n",
      "                     't1_ctd5ycu': Comment(id='ctd5ycu'),\n",
      "                     't1_ctdgeyj': Comment(id='ctdgeyj'),\n",
      "                     't1_cthwcdf': Comment(id='cthwcdf'),\n",
      "                     't1_ctvk404': Comment(id='ctvk404'),\n",
      "                     't1_ctx44kx': Comment(id='ctx44kx'),\n",
      "                     't1_ctx4em5': Comment(id='ctx4em5'),\n",
      "                     't1_ctxh7pl': Comment(id='ctxh7pl'),\n",
      "                     't1_ctyx6vg': Comment(id='ctyx6vg'),\n",
      "                     't1_ctz7auo': Comment(id='ctz7auo'),\n",
      "                     't1_ctzmebe': Comment(id='ctzmebe'),\n",
      "                     't1_cu5ykfz': Comment(id='cu5ykfz'),\n",
      "                     't1_cu8kmrn': Comment(id='cu8kmrn'),\n",
      "                     't1_cuawtww': Comment(id='cuawtww'),\n",
      "                     't1_cucnyw8': Comment(id='cucnyw8'),\n",
      "                     't1_cw9j1ef': Comment(id='cw9j1ef'),\n",
      "                     't1_cwe9ki7': Comment(id='cwe9ki7')},\n",
      " '_fetched': True,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x000002A844404E90>,\n",
      " 'all_awardings': [],\n",
      " 'allow_live_comments': True,\n",
      " 'approved_at_utc': None,\n",
      " 'approved_by': None,\n",
      " 'archived': False,\n",
      " 'author': Redditor(name='rram'),\n",
      " 'author_flair_background_color': None,\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_richtext': [],\n",
      " 'author_flair_template_id': None,\n",
      " 'author_flair_text': None,\n",
      " 'author_flair_text_color': None,\n",
      " 'author_flair_type': 'text',\n",
      " 'author_fullname': 't2_5wfps',\n",
      " 'author_is_blocked': False,\n",
      " 'author_patreon_flair': False,\n",
      " 'author_premium': False,\n",
      " 'awarders': [],\n",
      " 'banned_at_utc': None,\n",
      " 'banned_by': None,\n",
      " 'can_gild': False,\n",
      " 'can_mod_post': False,\n",
      " 'category': None,\n",
      " 'clicked': False,\n",
      " 'comment_limit': 2048,\n",
      " 'comment_sort': 'confidence',\n",
      " 'content_categories': None,\n",
      " 'contest_mode': False,\n",
      " 'created': 1434418540.0,\n",
      " 'created_utc': 1434418540.0,\n",
      " 'discussion_type': None,\n",
      " 'distinguished': 'admin',\n",
      " 'domain': 'self.redditdev',\n",
      " 'downs': 0,\n",
      " 'edited': 1440173665.0,\n",
      " 'gilded': 0,\n",
      " 'gildings': {},\n",
      " 'hidden': False,\n",
      " 'hide_score': False,\n",
      " 'id': '39zje0',\n",
      " 'is_created_from_ads_ui': False,\n",
      " 'is_crosspostable': False,\n",
      " 'is_meta': False,\n",
      " 'is_original_content': False,\n",
      " 'is_reddit_media_domain': False,\n",
      " 'is_robot_indexable': True,\n",
      " 'is_self': True,\n",
      " 'is_video': False,\n",
      " 'likes': None,\n",
      " 'link_flair_background_color': '',\n",
      " 'link_flair_css_class': '',\n",
      " 'link_flair_richtext': [],\n",
      " 'link_flair_text': 'Reddit API',\n",
      " 'link_flair_text_color': 'dark',\n",
      " 'link_flair_type': 'text',\n",
      " 'locked': False,\n",
      " 'media': None,\n",
      " 'media_embed': {},\n",
      " 'media_only': False,\n",
      " 'mod_note': None,\n",
      " 'mod_reason_by': None,\n",
      " 'mod_reason_title': None,\n",
      " 'mod_reports': [],\n",
      " 'name': 't3_39zje0',\n",
      " 'no_follow': False,\n",
      " 'num_comments': 117,\n",
      " 'num_crossposts': 0,\n",
      " 'num_duplicates': 0,\n",
      " 'num_reports': None,\n",
      " 'over_18': False,\n",
      " 'parent_whitelist_status': 'all_ads',\n",
      " 'permalink': '/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'pinned': False,\n",
      " 'post_hint': 'self',\n",
      " 'preview': {'enabled': False,\n",
      "             'images': [{'id': 'mKvBKwqPFmnxiYtLQRehhGDWhnrZdJVqzSL_7jJsHb4',\n",
      "                         'resolutions': [{'height': 150,\n",
      "                                          'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?width=108&crop=smart&auto=webp&v=enabled&s=5493b03128e098c72db6ac47d0bfb8f6e0d46c04',\n",
      "                                          'width': 108}],\n",
      "                         'source': {'height': 200,\n",
      "                                    'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?auto=webp&v=enabled&s=d110abbfdc90cdd2b6879c919367f906331134bb',\n",
      "                                    'width': 144},\n",
      "                         'variants': {}}]},\n",
      " 'pwls': 6,\n",
      " 'quarantine': False,\n",
      " 'removal_reason': None,\n",
      " 'removed_by': None,\n",
      " 'removed_by_category': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 269,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': 'Nearly 1 year ago we [gave you the ability to view reddit '\n",
      "             'completely over '\n",
      "             'SSL](http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html). '\n",
      "             \"Now we're ready to enforce that everyone use a secure connection \"\n",
      "             'with reddit.\\n'\n",
      "             '\\n'\n",
      "             '**Please ensure that all of your scripts can perform all of '\n",
      "             'their functions over HTTPS by June 29.** At this time we will '\n",
      "             'begin redirecting all site traffic to be over HTTPS and HTTP '\n",
      "             'will no longer be available.\\n'\n",
      "             '\\n'\n",
      "             'If this will be a problem for you, please let us know '\n",
      "             'immediately.\\n'\n",
      "             '\\n'\n",
      "             '**EDIT** 2015-08-21: IT IS DONE. You also have HSTS too.',\n",
      " 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Nearly 1 year ago we <a '\n",
      "                  'href=\"http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html\">gave '\n",
      "                  'you the ability to view reddit completely over SSL</a>. Now '\n",
      "                  'we&#39;re ready to enforce that everyone use a secure '\n",
      "                  'connection with reddit.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>Please ensure that all of your scripts can '\n",
      "                  'perform all of their functions over HTTPS by June '\n",
      "                  '29.</strong> At this time we will begin redirecting all '\n",
      "                  'site traffic to be over HTTPS and HTTP will no longer be '\n",
      "                  'available.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>If this will be a problem for you, please let us know '\n",
      "                  'immediately.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>EDIT</strong> 2015-08-21: IT IS DONE. You also '\n",
      "                  'have HSTS too.</p>\\n'\n",
      "                  '</div><!-- SC_ON -->',\n",
      " 'send_replies': True,\n",
      " 'spoiler': False,\n",
      " 'stickied': False,\n",
      " 'subreddit': Subreddit(display_name='redditdev'),\n",
      " 'subreddit_id': 't5_2qizd',\n",
      " 'subreddit_name_prefixed': 'r/redditdev',\n",
      " 'subreddit_subscribers': 72082,\n",
      " 'subreddit_type': 'public',\n",
      " 'suggested_sort': None,\n",
      " 'thumbnail': 'self',\n",
      " 'thumbnail_height': None,\n",
      " 'thumbnail_width': None,\n",
      " 'title': 'reddit will soon only be available over HTTPS',\n",
      " 'top_awarded_type': None,\n",
      " 'total_awards_received': 0,\n",
      " 'treatment_tags': [],\n",
      " 'ups': 269,\n",
      " 'upvote_ratio': 0.97,\n",
      " 'url': 'https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'user_reports': [],\n",
      " 'view_count': None,\n",
      " 'visited': False,\n",
      " 'whitelist_status': 'all_ads',\n",
      " 'wls': 6}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# assume you have a praw.Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(\"39zje0\")\n",
    "print(submission.title)  # to make it non-lazy\n",
    "pprint.pprint(vars(submission))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearning\n",
      "{'_comments': <praw.models.comment_forest.CommentForest object at 0x000002A843E65C90>,\n",
      " '_comments_by_id': {'t1_cs7vwlm': Comment(id='cs7vwlm'),\n",
      "                     't1_cs7xcx2': Comment(id='cs7xcx2'),\n",
      "                     't1_cs7ykx6': Comment(id='cs7ykx6'),\n",
      "                     't1_cs81mem': Comment(id='cs81mem'),\n",
      "                     't1_cs81xp8': Comment(id='cs81xp8'),\n",
      "                     't1_cs82epc': Comment(id='cs82epc'),\n",
      "                     't1_cs82kes': Comment(id='cs82kes'),\n",
      "                     't1_cs83dd8': Comment(id='cs83dd8'),\n",
      "                     't1_cs83ua8': Comment(id='cs83ua8'),\n",
      "                     't1_cs83xhc': Comment(id='cs83xhc'),\n",
      "                     't1_cs846jk': Comment(id='cs846jk'),\n",
      "                     't1_cs847yp': Comment(id='cs847yp'),\n",
      "                     't1_cs848n2': Comment(id='cs848n2'),\n",
      "                     't1_cs84apf': Comment(id='cs84apf'),\n",
      "                     't1_cs84kz5': Comment(id='cs84kz5'),\n",
      "                     't1_cs84n50': Comment(id='cs84n50'),\n",
      "                     't1_cs84poe': Comment(id='cs84poe'),\n",
      "                     't1_cs84rjx': Comment(id='cs84rjx'),\n",
      "                     't1_cs85lql': Comment(id='cs85lql'),\n",
      "                     't1_cs87jic': Comment(id='cs87jic'),\n",
      "                     't1_cs898tl': Comment(id='cs898tl'),\n",
      "                     't1_cs89jvc': Comment(id='cs89jvc'),\n",
      "                     't1_cs8a7pr': Comment(id='cs8a7pr'),\n",
      "                     't1_cs8blse': Comment(id='cs8blse'),\n",
      "                     't1_cs8feqw': Comment(id='cs8feqw'),\n",
      "                     't1_cs8hwfi': Comment(id='cs8hwfi'),\n",
      "                     't1_cs8i7od': Comment(id='cs8i7od'),\n",
      "                     't1_cs8iofx': Comment(id='cs8iofx'),\n",
      "                     't1_cs8j88h': Comment(id='cs8j88h'),\n",
      "                     't1_cs8jh7g': Comment(id='cs8jh7g'),\n",
      "                     't1_cs8jkru': Comment(id='cs8jkru'),\n",
      "                     't1_cs8jns1': Comment(id='cs8jns1'),\n",
      "                     't1_cs8jq4l': Comment(id='cs8jq4l'),\n",
      "                     't1_cs8kbbq': Comment(id='cs8kbbq'),\n",
      "                     't1_cs8kc4z': Comment(id='cs8kc4z'),\n",
      "                     't1_cs8kf07': Comment(id='cs8kf07'),\n",
      "                     't1_cs8km94': Comment(id='cs8km94'),\n",
      "                     't1_cs8knnt': Comment(id='cs8knnt'),\n",
      "                     't1_cs8koyi': Comment(id='cs8koyi'),\n",
      "                     't1_cs8kwd0': Comment(id='cs8kwd0'),\n",
      "                     't1_cs8kxvj': Comment(id='cs8kxvj'),\n",
      "                     't1_cs8l1wk': Comment(id='cs8l1wk'),\n",
      "                     't1_cs8lkbl': Comment(id='cs8lkbl'),\n",
      "                     't1_cs8llxd': Comment(id='cs8llxd'),\n",
      "                     't1_cs8lso1': Comment(id='cs8lso1'),\n",
      "                     't1_cs8mez7': Comment(id='cs8mez7'),\n",
      "                     't1_cs8mhlc': Comment(id='cs8mhlc'),\n",
      "                     't1_cs8p6nn': Comment(id='cs8p6nn'),\n",
      "                     't1_cs8t8u9': Comment(id='cs8t8u9'),\n",
      "                     't1_cs8ts1k': Comment(id='cs8ts1k'),\n",
      "                     't1_cs8x1s6': Comment(id='cs8x1s6'),\n",
      "                     't1_cs905rb': Comment(id='cs905rb'),\n",
      "                     't1_cs974f0': Comment(id='cs974f0'),\n",
      "                     't1_cs9daoa': Comment(id='cs9daoa'),\n",
      "                     't1_cs9dn0k': Comment(id='cs9dn0k'),\n",
      "                     't1_cs9dr4n': Comment(id='cs9dr4n'),\n",
      "                     't1_cs9e0cr': Comment(id='cs9e0cr'),\n",
      "                     't1_cs9ecqc': Comment(id='cs9ecqc'),\n",
      "                     't1_cs9emjk': Comment(id='cs9emjk'),\n",
      "                     't1_cs9gpm0': Comment(id='cs9gpm0'),\n",
      "                     't1_cs9i9q3': Comment(id='cs9i9q3'),\n",
      "                     't1_cs9l636': Comment(id='cs9l636'),\n",
      "                     't1_cs9nc77': Comment(id='cs9nc77'),\n",
      "                     't1_cs9ndy5': Comment(id='cs9ndy5'),\n",
      "                     't1_cs9no26': Comment(id='cs9no26'),\n",
      "                     't1_cs9nweg': Comment(id='cs9nweg'),\n",
      "                     't1_cs9rfqm': Comment(id='cs9rfqm'),\n",
      "                     't1_cs9tp7a': Comment(id='cs9tp7a'),\n",
      "                     't1_cs9tso1': Comment(id='cs9tso1'),\n",
      "                     't1_cs9z8jp': Comment(id='cs9z8jp'),\n",
      "                     't1_csa3tdo': Comment(id='csa3tdo'),\n",
      "                     't1_csa5ckt': Comment(id='csa5ckt'),\n",
      "                     't1_csa7guj': Comment(id='csa7guj'),\n",
      "                     't1_csaf2xj': Comment(id='csaf2xj'),\n",
      "                     't1_csaifdc': Comment(id='csaifdc'),\n",
      "                     't1_csalcjw': Comment(id='csalcjw'),\n",
      "                     't1_csapdsx': Comment(id='csapdsx'),\n",
      "                     't1_csaqqg3': Comment(id='csaqqg3'),\n",
      "                     't1_csav2lc': Comment(id='csav2lc'),\n",
      "                     't1_csay9va': Comment(id='csay9va'),\n",
      "                     't1_csaym10': Comment(id='csaym10'),\n",
      "                     't1_csaz9yt': Comment(id='csaz9yt'),\n",
      "                     't1_csazbjn': Comment(id='csazbjn'),\n",
      "                     't1_csb01td': Comment(id='csb01td'),\n",
      "                     't1_csb0xnn': Comment(id='csb0xnn'),\n",
      "                     't1_csb2iss': Comment(id='csb2iss'),\n",
      "                     't1_csbxpsm': Comment(id='csbxpsm'),\n",
      "                     't1_csc3ini': Comment(id='csc3ini'),\n",
      "                     't1_csghdd8': Comment(id='csghdd8'),\n",
      "                     't1_csh2ggk': Comment(id='csh2ggk'),\n",
      "                     't1_cshasj0': Comment(id='cshasj0'),\n",
      "                     't1_csjbsq9': Comment(id='csjbsq9'),\n",
      "                     't1_cso07fm': Comment(id='cso07fm'),\n",
      "                     't1_csocuyr': Comment(id='csocuyr'),\n",
      "                     't1_csomokx': Comment(id='csomokx'),\n",
      "                     't1_csoncci': Comment(id='csoncci'),\n",
      "                     't1_csoqfop': Comment(id='csoqfop'),\n",
      "                     't1_csoqwaj': Comment(id='csoqwaj'),\n",
      "                     't1_csotkw8': Comment(id='csotkw8'),\n",
      "                     't1_csovx2n': Comment(id='csovx2n'),\n",
      "                     't1_csow6zt': Comment(id='csow6zt'),\n",
      "                     't1_csoycql': Comment(id='csoycql'),\n",
      "                     't1_csp2uvk': Comment(id='csp2uvk'),\n",
      "                     't1_csrxl8t': Comment(id='csrxl8t'),\n",
      "                     't1_csrxya2': Comment(id='csrxya2'),\n",
      "                     't1_cswg4ku': Comment(id='cswg4ku'),\n",
      "                     't1_cswg9cd': Comment(id='cswg9cd'),\n",
      "                     't1_cswk16a': Comment(id='cswk16a'),\n",
      "                     't1_ct3lvx5': Comment(id='ct3lvx5'),\n",
      "                     't1_ct4hrif': Comment(id='ct4hrif'),\n",
      "                     't1_ctd5ycu': Comment(id='ctd5ycu'),\n",
      "                     't1_ctdgeyj': Comment(id='ctdgeyj'),\n",
      "                     't1_cthwcdf': Comment(id='cthwcdf'),\n",
      "                     't1_ctvk404': Comment(id='ctvk404'),\n",
      "                     't1_ctx44kx': Comment(id='ctx44kx'),\n",
      "                     't1_ctx4em5': Comment(id='ctx4em5'),\n",
      "                     't1_ctxh7pl': Comment(id='ctxh7pl'),\n",
      "                     't1_ctyx6vg': Comment(id='ctyx6vg'),\n",
      "                     't1_ctz7auo': Comment(id='ctz7auo'),\n",
      "                     't1_ctzmebe': Comment(id='ctzmebe'),\n",
      "                     't1_cu5ykfz': Comment(id='cu5ykfz'),\n",
      "                     't1_cu8kmrn': Comment(id='cu8kmrn'),\n",
      "                     't1_cuawtww': Comment(id='cuawtww'),\n",
      "                     't1_cucnyw8': Comment(id='cucnyw8'),\n",
      "                     't1_cw9j1ef': Comment(id='cw9j1ef'),\n",
      "                     't1_cwe9ki7': Comment(id='cwe9ki7')},\n",
      " '_fetched': True,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x000002A844404E90>,\n",
      " 'all_awardings': [],\n",
      " 'allow_live_comments': True,\n",
      " 'approved_at_utc': None,\n",
      " 'approved_by': None,\n",
      " 'archived': False,\n",
      " 'author': Redditor(name='rram'),\n",
      " 'author_flair_background_color': None,\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_richtext': [],\n",
      " 'author_flair_template_id': None,\n",
      " 'author_flair_text': None,\n",
      " 'author_flair_text_color': None,\n",
      " 'author_flair_type': 'text',\n",
      " 'author_fullname': 't2_5wfps',\n",
      " 'author_is_blocked': False,\n",
      " 'author_patreon_flair': False,\n",
      " 'author_premium': False,\n",
      " 'awarders': [],\n",
      " 'banned_at_utc': None,\n",
      " 'banned_by': None,\n",
      " 'can_gild': False,\n",
      " 'can_mod_post': False,\n",
      " 'category': None,\n",
      " 'clicked': False,\n",
      " 'comment_limit': 2048,\n",
      " 'comment_sort': 'confidence',\n",
      " 'content_categories': None,\n",
      " 'contest_mode': False,\n",
      " 'created': 1434418540.0,\n",
      " 'created_utc': 1434418540.0,\n",
      " 'discussion_type': None,\n",
      " 'distinguished': 'admin',\n",
      " 'domain': 'self.redditdev',\n",
      " 'downs': 0,\n",
      " 'edited': 1440173665.0,\n",
      " 'gilded': 0,\n",
      " 'gildings': {},\n",
      " 'hidden': False,\n",
      " 'hide_score': False,\n",
      " 'id': '39zje0',\n",
      " 'is_created_from_ads_ui': False,\n",
      " 'is_crosspostable': False,\n",
      " 'is_meta': False,\n",
      " 'is_original_content': False,\n",
      " 'is_reddit_media_domain': False,\n",
      " 'is_robot_indexable': True,\n",
      " 'is_self': True,\n",
      " 'is_video': False,\n",
      " 'likes': None,\n",
      " 'link_flair_background_color': '',\n",
      " 'link_flair_css_class': '',\n",
      " 'link_flair_richtext': [],\n",
      " 'link_flair_text': 'Reddit API',\n",
      " 'link_flair_text_color': 'dark',\n",
      " 'link_flair_type': 'text',\n",
      " 'locked': False,\n",
      " 'media': None,\n",
      " 'media_embed': {},\n",
      " 'media_only': False,\n",
      " 'mod_note': None,\n",
      " 'mod_reason_by': None,\n",
      " 'mod_reason_title': None,\n",
      " 'mod_reports': [],\n",
      " 'name': 't3_39zje0',\n",
      " 'no_follow': False,\n",
      " 'num_comments': 117,\n",
      " 'num_crossposts': 0,\n",
      " 'num_duplicates': 0,\n",
      " 'num_reports': None,\n",
      " 'over_18': False,\n",
      " 'parent_whitelist_status': 'all_ads',\n",
      " 'permalink': '/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'pinned': False,\n",
      " 'post_hint': 'self',\n",
      " 'preview': {'enabled': False,\n",
      "             'images': [{'id': 'mKvBKwqPFmnxiYtLQRehhGDWhnrZdJVqzSL_7jJsHb4',\n",
      "                         'resolutions': [{'height': 150,\n",
      "                                          'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?width=108&crop=smart&auto=webp&v=enabled&s=5493b03128e098c72db6ac47d0bfb8f6e0d46c04',\n",
      "                                          'width': 108}],\n",
      "                         'source': {'height': 200,\n",
      "                                    'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?auto=webp&v=enabled&s=d110abbfdc90cdd2b6879c919367f906331134bb',\n",
      "                                    'width': 144},\n",
      "                         'variants': {}}]},\n",
      " 'pwls': 6,\n",
      " 'quarantine': False,\n",
      " 'removal_reason': None,\n",
      " 'removed_by': None,\n",
      " 'removed_by_category': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 269,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': 'Nearly 1 year ago we [gave you the ability to view reddit '\n",
      "             'completely over '\n",
      "             'SSL](http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html). '\n",
      "             \"Now we're ready to enforce that everyone use a secure connection \"\n",
      "             'with reddit.\\n'\n",
      "             '\\n'\n",
      "             '**Please ensure that all of your scripts can perform all of '\n",
      "             'their functions over HTTPS by June 29.** At this time we will '\n",
      "             'begin redirecting all site traffic to be over HTTPS and HTTP '\n",
      "             'will no longer be available.\\n'\n",
      "             '\\n'\n",
      "             'If this will be a problem for you, please let us know '\n",
      "             'immediately.\\n'\n",
      "             '\\n'\n",
      "             '**EDIT** 2015-08-21: IT IS DONE. You also have HSTS too.',\n",
      " 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Nearly 1 year ago we <a '\n",
      "                  'href=\"http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html\">gave '\n",
      "                  'you the ability to view reddit completely over SSL</a>. Now '\n",
      "                  'we&#39;re ready to enforce that everyone use a secure '\n",
      "                  'connection with reddit.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>Please ensure that all of your scripts can '\n",
      "                  'perform all of their functions over HTTPS by June '\n",
      "                  '29.</strong> At this time we will begin redirecting all '\n",
      "                  'site traffic to be over HTTPS and HTTP will no longer be '\n",
      "                  'available.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>If this will be a problem for you, please let us know '\n",
      "                  'immediately.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>EDIT</strong> 2015-08-21: IT IS DONE. You also '\n",
      "                  'have HSTS too.</p>\\n'\n",
      "                  '</div><!-- SC_ON -->',\n",
      " 'send_replies': True,\n",
      " 'spoiler': False,\n",
      " 'stickied': False,\n",
      " 'subreddit': Subreddit(display_name='redditdev'),\n",
      " 'subreddit_id': 't5_2qizd',\n",
      " 'subreddit_name_prefixed': 'r/redditdev',\n",
      " 'subreddit_subscribers': 72082,\n",
      " 'subreddit_type': 'public',\n",
      " 'suggested_sort': None,\n",
      " 'thumbnail': 'self',\n",
      " 'thumbnail_height': None,\n",
      " 'thumbnail_width': None,\n",
      " 'title': 'reddit will soon only be available over HTTPS',\n",
      " 'top_awarded_type': None,\n",
      " 'total_awards_received': 0,\n",
      " 'treatment_tags': [],\n",
      " 'ups': 269,\n",
      " 'upvote_ratio': 0.97,\n",
      " 'url': 'https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'user_reports': [],\n",
      " 'view_count': None,\n",
      " 'visited': False,\n",
      " 'whitelist_status': 'all_ads',\n",
      " 'wls': 6}\n"
     ]
    }
   ],
   "source": [
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "print(ml_subreddit)  # to make it non-lazy\n",
    "pprint.pprint(vars(submission))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab A specfic User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_listing_use_sort': True, 'name': 'lpurgsl', '_reddit': <praw.reddit.Reddit object at 0x000002A844404E90>, '_fetched': False}\n",
      "{'_fetched': False,\n",
      " '_listing_use_sort': True,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x000002A844404E90>,\n",
      " 'name': 'lpurgsl'}\n",
      "<praw.models.listing.mixins.redditor.SubListing object at 0x000002A845C1B490>\n"
     ]
    }
   ],
   "source": [
    "user = reddit.redditor('lpurgsl')\n",
    "print(vars(user))\n",
    "pprint.pprint(vars(user))\n",
    "\n",
    "submissions = user.submissions\n",
    "print(submissions)\n",
    "# https://praw.readthedocs.io/en/stable/code_overview/other/sublisting.html#praw.models.listing.mixins.redditor.SubListing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subreddit(display_name='15minutefood'), Subreddit(display_name='18_19'), Subreddit(display_name='100yearsago'), Subreddit(display_name='18nsfw'), Subreddit(display_name='13or30'), Subreddit(display_name='196'), Subreddit(display_name='1200isplenty'), Subreddit(display_name='1000ccplus'), Subreddit(display_name='1500isplenty')]\n",
      "[Subreddit(display_name='MachineLearning'), Subreddit(display_name='machinelearningmemes'), Subreddit(display_name='machinelearningnews'), Subreddit(display_name='machinelearningart'), Subreddit(display_name='MachineLearningJobs'), Subreddit(display_name='MachineLearningDervs'), Subreddit(display_name='MachineLearningCollab'), Subreddit(display_name='MachineLearning_JP'), Subreddit(display_name='machineLearning101'), Subreddit(display_name='MachineLearningKeras')]\n",
      "<class 'praw.models.reddit.subreddit.Subreddit'>\n",
      "matchy\n"
     ]
    }
   ],
   "source": [
    "print(reddit.subreddits.search_by_name('1'))\n",
    "print(reddit.subreddits.search_by_name('MachineLearning'))\n",
    "listyy = reddit.subreddits.search_by_name('MachineLearning')\n",
    "print(type(listyy[0]))\n",
    "if listyy[0] == 'MachineLearning':\n",
    "    print('matchy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-collect-a-reddit-dataset-c369de539114\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\":\"[D] Simple Questions Thread\",\n",
      "        \"score\":57,\n",
      "        \"id\":\"12wcr8i\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/12wcr8i\\/d_simple_questions_thread\\/\",\n",
      "        \"num_comments\":148,\n",
      "        \"body\":\"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\\n\\nThread will stay alive until next one so keep posting after the date in the title.\\n\\nThanks to everyone for answering questions in the previous thread!\",\n",
      "        \"created\":1682262020.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"Reminder: Use the report button and read the rules!\",\n",
      "        \"score\":48,\n",
      "        \"id\":\"120f4oy\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/120f4oy\\/reminder_use_the_report_button_and_read_the_rules\\/\",\n",
      "        \"num_comments\":0,\n",
      "        \"body\":\"\",\n",
      "        \"created\":1679650349.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] Google \\\"We Have No Moat, And Neither Does OpenAI\\\": Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI\",\n",
      "        \"score\":667,\n",
      "        \"id\":\"137rxgw\",\n",
      "        \"url\":\"https:\\/\\/www.semianalysis.com\\/p\\/google-we-have-no-moat-and-neither\",\n",
      "        \"num_comments\":113,\n",
      "        \"body\":\"\",\n",
      "        \"created\":1683216810.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[R] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\",\n",
      "        \"score\":28,\n",
      "        \"id\":\"1381gd3\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1381gd3\\/r_distilling_stepbystep_outperforming_larger\\/\",\n",
      "        \"num_comments\":8,\n",
      "        \"body\":\"paper:  [\\\\[2305.02301\\\\] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arxiv.org)](https:\\/\\/arxiv.org\\/abs\\/2305.02301) \\n\\nAbstract: \\n\\n> Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled\\/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80% of available data on a benchmark task.\",\n",
      "        \"created\":1683237630.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[R] Fully Autonomous Programming with Large Language Models\",\n",
      "        \"score\":27,\n",
      "        \"id\":\"137odqz\",\n",
      "        \"url\":\"https:\\/\\/arxiv.org\\/abs\\/2304.10423\",\n",
      "        \"num_comments\":7,\n",
      "        \"body\":\"\",\n",
      "        \"created\":1683212038.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[Discussion]: Mark Zuckerberg on Meta's Strategy on Open Source and AI during the earnings call\",\n",
      "        \"score\":386,\n",
      "        \"id\":\"1373nhq\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1373nhq\\/discussion_mark_zuckerberg_on_metas_strategy_on\\/\",\n",
      "        \"num_comments\":79,\n",
      "        \"body\":\"During  the recent earnings call, Mark Zuckerberg answered a question from Eric  Sheridan of Goldman Sachs on Meta's AI strategy, opportunities to  integrate into products, and why they open source models and how it  would benefit their business.\\n\\nI found the reasoning to be very sound and promising for the OSS and AI community.\\n\\nThe  biggest risk from AI, in my opinion, is not the doomsday scenarios that  intuitively come to mind but rather that the most powerful AI systems  will only be accessible to the most powerful and resourceful  corporations.\\n\\nQuote copied from Ben Thompson's write up on Meta's earning in his [Stratechery blog post](https:\\/\\/stratechery.com\\/2023\\/facebook-earnings-generative-ai-and-messaging-monetization-open-source-and-ai\\/) which goes beyond AI. *It's behind a paywall but I highly recommend it personally.*\\n\\nSome noteworthy quotes that signal the thought process at Meta FAIR and more broadly\\n\\n* We\\u2019re just playing a different game on the infrastructure  than companies like Google or Microsoft or Amazon\\n* We would aspire to and hope to make even more open than that. So, we\\u2019ll need to figure out a way to do that.\\n* ...lead us to do more work in terms of open sourcing, some of the lower level models and tools\\n* Open sourcing low level tools make the way we run all this infrastructure more efficient over time.\\n* On  PyTorch: It\\u2019s generally been very valuable for us to provide that  because now  all of the best developers across the industry are using  tools that  we\\u2019re also using internally.\\n* I would expect us to be pushing and helping  to build out an open ecosystem.\\n\\nFor  all the negative that comes out of the popular discourse on Meta, I  think their work to open source key tech tools over the last 10 years  has been exceptional, here's hoping it continues into this decade of AI  and pushes other tech giants to also realize the benefits of Open  Source.\\n\\nFull Transcript:\\n\\n>Right  now most of the companies that are training large language  models have  business models that lead them to a closed approach to development. I  think **there\\u2019s an** **important opportunity to help create an  open ecosystem.**  If we can help be a part of this, then much of the  industry will  standardize on using these open tools and help improve  them further. So  this will make it easier for other companies to  integrate with our  products and platforms as we enable more  integrations, and that will  help our products stay at the leading edge  as well.  \\nOur  approach to AI and our infrastructure has always been fairly  open. We  open source many of our state of the art models so people can   experiment and build with them. This quarter we released our LLaMa LLM   to researchers. It has 65 billion parameters but outperforms larger   models and has proven quite popular. We\\u2019ve also open-sourced three other   groundbreaking visual models along with their training data and model   weights \\u2014 Segment Anything, DinoV2, and our Animated Drawings tool \\u2014  and  we\\u2019ve gotten positive feedback on all of those as well.  \\nI  think that there\\u2019s an important distinction between the products we  offer and a lot of the technical infrastructure, especially the software  that we write to support that. And historically, whether it\\u2019s the Open  Compute project that we\\u2019ve done or just open sourcing a lot of the   infrastructure that we\\u2019ve built, we\\u2019ve historically open sourced a lot   of that infrastructure, even though we haven\\u2019t open sourced the code for   our core products or anything like that.  \\nAnd the reason why I think why we do this is that unlike some of  the other companies in the space, **we\\u2019re not selling a cloud computing service** **where we try to keep the different software infrastructure that we\\u2019re building proprietary.** For us, **it\\u2019s way better if the industry  standardizes on the basic tools that we\\u2019re using**  and therefore we can benefit from the improvements that others make and  others\\u2019 use of those tools can, in some cases like Open Compute, **drive down the costs** of  those things which make our business more efficient too. So I think to  some degree **we\\u2019re just playing a different game** on the infrastructure  than companies like Google or Microsoft or Amazon, and that creates different incentives for us.  \\nSo overall, I think **that that\\u2019s going to lead us to do more work in terms of open sourcing, some of the lower level models and tools**.  But of  course, a lot of the product work itself is going to be  specific and  integrated with the things that we do. So it\\u2019s not that  everything we do is going to be open. Obviously, a bunch of this needs  to be developed in a way that creates unique value for our products, but  I think in  terms of the basic models, **I would expect us to be pushing and helping  to build out an open ecosystem** here, which I think is something that\\u2019s  going to be important.  \\nOn the AI tools, and we have a bunch of history here, right? So if you  if you look at what we\\u2019ve done with **PyTorch**,  for example, which has  generally become the standard in the industry  as a tool that a lot of  folks who are building AI models and different  things in that space use,  **it\\u2019s generally been very valuable** for us to provide that because now  all of the **best developers across the industry are using tools that  we\\u2019re also using internally**.  So the tool chain is the same. So when they create some innovation, we  can easily integrate it into the things that we\\u2019re doing. When we  improve something, it improves other products too. Because it\\u2019s  integrated with our technology stack, when there are opportunities to  make integrations with products, it\\u2019s much easier to  make sure that  developers and other folks are compatible with the things  that we need  in the way that our systems work.  \\nSo there are a lot of advantages, but **I view this more as a kind of back end infrastructure advantage with potential integrations on the  product side**,  but one that should hopefully enable us to stay at the  leading edge  and integrate more broadly with the community and also make  the way we  run all this infrastructure more efficient over time. There  are a  number of models. I just gave PyTorch as an example. Open Compute  is  another model that has worked really well for us in this way, both to   incorporate both innovation and scale efficiency into our own   infrastructure.  \\nSo I think that  there\\u2019s, our incentives I think are basically  aligned towards moving in  this direction. Now that said, there\\u2019s a lot  to figure out, right? So  when you asked if there are going to be other opportunities, I hope so. I  can\\u2019t speak to what all those things might  be now. This is all quite  early in getting developed. **The better we do at the foundational work, the more opportunities** I think that will come and present themselves. So I think that that\\u2019s all stuff that we need to  figure out. But at least **at the base level, I think we\\u2019re generally incentivized to move in this direction**. And we also need to figure out  how to go in that direction over time.  \\nI  mean, I mentioned LLaMA before and I also want to be clear that  while  I\\u2019m talking about helping contribute to an open ecosystem, LLaMA  is a  model that we only really made available to researchers and there\\u2019s  a  lot of really good stuff that\\u2019s happening there. But a lot of the  work  that we\\u2019re doing, I think, **we would aspire to and hope to make even more open than that. So, we\\u2019ll need to figure out a way to do that.**\",\n",
      "        \"created\":1683157697.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"Prediction and Entropy of Printed English. Shannon 1951\",\n",
      "        \"score\":5,\n",
      "        \"id\":\"137zz3j\",\n",
      "        \"url\":\"https:\\/\\/archive.org\\/details\\/bstj30-1-50\",\n",
      "        \"num_comments\":2,\n",
      "        \"body\":\"This is a great and easily read paper. LLMs do the task described here really well. And I didn't realize how useful that could be.\",\n",
      "        \"created\":1683234359.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] What tech stacks do you use when creating an LLM based app?\",\n",
      "        \"score\":2,\n",
      "        \"id\":\"1386pjt\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/1386pjt\\/d_what_tech_stacks_do_you_use_when_creating_an\\/\",\n",
      "        \"num_comments\":0,\n",
      "        \"body\":\"Making apps based on foundational LLMs feels like it should have a tech stack \\\"pattern\\\" - a commonly used set of tools that most of the apps use unless there's a unique reason to deviate. \\n\\nWhat tech stacks do people here use?\\n\\nThe link below has some suggestions, but it would be great to know what people use in practice. Are these ones good?\\n\\n[https:\\/\\/gradientflow.com\\/building-llm-powered-apps-what-you-need-to-know\\/](https:\\/\\/gradientflow.com\\/building-llm-powered-apps-what-you-need-to-know\\/)\",\n",
      "        \"created\":1683250596.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[D] Can biological neurons be properly emulated with current microcomputer hardware?\",\n",
      "        \"score\":0,\n",
      "        \"id\":\"13890af\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/13890af\\/d_can_biological_neurons_be_properly_emulated\\/\",\n",
      "        \"num_comments\":1,\n",
      "        \"body\":\"I've been doing some browsing on how neurons work and what follows is the conclusion I've come to.\\n\\nThe functionality of biological neurons is impossible to emulate with current microcomputing technology. This is because biological neurons have 2 important features that are expensive to imitate:\\n\\n1.\\tIt is possible for any two biological neurons to connect. Since their cell body, along with their axons and dendrites, is able to move freely, two correlated neurons will eventually find and connect to each other if given enough time. The only way to mimic this behavior in a single-processor computer without sacrificing time is by making a fully connected graph of the neurons, which is awful because it requires n\\\\^2 space.\\n\\n2.\\tEach neuron operates in parallel. This means increases in number of neurons only require more mass, which is much more freely available than the extra time that a single-processor computer would need to add the same number of neurons. For instance, the human brain has \\\\~86 billion neurons. Assuming a 1 GHz oscillator, and that each neuron only requires 1 cycle to calculate its value, a single-processor computer would still take a whole 8.6 seconds to calculate the state of the brain after 1 time step. The human brain runs the same time step in, well, much less time than that.\\n\\nSo basically, in order to emulate a brain, a single-processor computer would have to make some tradeoff between n\\\\^2 space and n time, neither of which can be afforded.\\n\\nThoughts?\",\n",
      "        \"created\":1683256415.0\n",
      "    },\n",
      "    {\n",
      "        \"title\":\"[Research] Towards Accurate, Credible and Traceable Large Language Models\\uff01\\uff01\\uff01\",\n",
      "        \"score\":11,\n",
      "        \"id\":\"137iyxk\",\n",
      "        \"url\":\"https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/137iyxk\\/research_towards_accurate_credible_and_traceable\\/\",\n",
      "        \"num_comments\":4,\n",
      "        \"body\":\"Hello everyone, in this paper, we propose a novel method to combine Large Language Models with Information Retrieval  to improve the accuracy, credibility and traceability of LLM-generated content!\\n\\nPaper: [https:\\/\\/arxiv.org\\/abs\\/2304.14732](https:\\/\\/arxiv.org\\/abs\\/2304.14732)\\n\\n&#x200B;\\n\\nhttps:\\/\\/preview.redd.it\\/t5kdmrna3txa1.png?width=1431&format=png&auto=webp&v=enabled&s=fa52e9bd9f9d5ae892509f551f1ef63234bb77ff\",\n",
      "        \"created\":1683202882.0\n",
      "    }\n",
      "]\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "def selected_subReddit():\n",
    "    subreddit_name = input(\"\\ninput just the subreddit name\\n Ex. if you want \\\"r/MachineLearning\\\" only input \\\"MachineLearning\\\"\\n\")\n",
    "    \n",
    "    subreddit_name = selected_subRedditNameChecker(subreddit_name) # returns zero or reddit instance of type subreddit\n",
    "    if subreddit_name == 0:\n",
    "        return 0;\n",
    "\n",
    "    posts = []    \n",
    "    for post in subreddit_name.hot(limit=10):\n",
    "        posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "    df = pd.DataFrame(posts, columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "    json_info = df.to_json(orient='records', indent=4)\n",
    "    return json_info\n",
    "\n",
    "def selected_subRedditNameChecker(subreddit_name): #checks if user inputted name is valid... if not gives top three matches, returns 0 or reddit type of subredit\n",
    "    checking_name = reddit.subreddits.search_by_name(subreddit_name)\n",
    "    if subreddit_name != checking_name[0]:\n",
    "        print(\"\\nNo exact match found!\\nDid you mean\\n\",checking_name[0],\" or \", checking_name[1], \" or \", checking_name[2], \"\\nPlease try again!!\\n\")\n",
    "        return 0\n",
    "    # print(type(checking_name[0]))\n",
    "    return checking_name[0]\n",
    "\n",
    "# Using the special variable \n",
    "# __name__\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=\"3IS_PPpIX3IkVIh-1f8cHQ\",\n",
    "    client_secret=\"AvAKHMywUgLyGlVSVD0bQMRpZbhb1w\",\n",
    "    user_agent =\"crawler/scrapper\")\n",
    "    menu = '''\\nHello, Welcome to r\\'Crawler\\n\\nPlease select an NUMBER option\\n\\nEnter 1 to search for a subredit\\nEnter 2 to search for a reddit user\\nEnter 3 to search for a xyz\\n'''\n",
    "    init = input(menu)\n",
    "    file = '['\n",
    "    while init !=0:\n",
    "        match init:\n",
    "            case '1':\n",
    "                output = selected_subReddit()\n",
    "                if output != 0:  \n",
    "                    print(output)\n",
    "                    file += output\n",
    "                init = input(menu)\n",
    "            case '2':\n",
    "                print(\"2\")\n",
    "                init = input('\\n\\nPlease select an option\\n')\n",
    "\n",
    "            case '3':\n",
    "                print(\"3\")\n",
    "                init = input('\\n\\nPlease select an option\\n')\n",
    "            \n",
    "            case '4':\n",
    "                print(\"4\")\n",
    "                init = input('\\n\\nPlease select an option\\n')\n",
    "\n",
    "            case '5':\n",
    "                print(\"5\")\n",
    "                init = input('\\n\\nPlease select an option\\n')\n",
    "            case '10':\n",
    "                file +='\\n]'\n",
    "                with open(\"sample.json\", \"w\") as outfile:\n",
    "                    json.dump(json.loads(file), outfile, indent = 4)\n",
    "                print(\"bye\")\n",
    "                init = 0\n",
    "            case _:\n",
    "                print(\"default case statement\")\n",
    "                init = input('\\nHello, Welcome to r\\'Crawler\\n\\nPlease enter a valid number an option\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
